{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380f9403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import csv\n",
    "import random\n",
    "import os\n",
    "from io import StringIO\n",
    "import keras\n",
    "from keras.layers import Input, Dense, concatenate, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras import backend as K\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sparsemax import Sparsemax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, roc_curve, f1_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01852fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Define how you want to encode ##########################\n",
    "\n",
    "#The path used to get the HLA dictionary：\n",
    "aa_dict_dir='../library/Atchley_factors.csv'\n",
    "hla_db_dir='../library/hla_library'\n",
    "aa_list = [letter for letter in 'ARNDCEQGHILKMFPSTWYV']\n",
    "aa_to_idx_dict = {aa: idx+1 for idx, aa in enumerate(aa_list)}\n",
    "aa_dict = {aa: idx+1 for idx, aa in enumerate(aa_list)}\n",
    "\n",
    "pca15_aaidx = {\n",
    "    'A': np.array([\n",
    "        -0.97090603, -0.32368135, 15.72065182, -0.50884066,  3.74021858,\n",
    "        -0.77879681,  3.38667656, -0.91306275,  3.00614751, -2.32919906,\n",
    "         0.7870219 , -1.87437966,  1.53335388,  1.344461  ,  3.35815929]),\n",
    "    'R': np.array([\n",
    "         8.53814625, -13.78745836,  -4.7706284 ,  -1.16449378,\n",
    "        -9.46561383,   6.79244266,   1.98971004,  -5.15274046,\n",
    "        -2.93772088,  -5.89206951,   5.26993696,   1.61036613,\n",
    "        -1.95208183,  -0.64409   ,   1.53934309]),\n",
    "    'N': np.array([\n",
    "        14.86976088,  1.57062419, -3.62321751,  5.63672398, -1.78396131,\n",
    "        -3.62736822, -1.44652457,  5.39945262,  0.87616878,  4.1128776 ,\n",
    "         1.91381611,  2.4206327 , -3.81386422, -5.242596  ,  3.97131233]),\n",
    "    'D': np.array([\n",
    "        18.12626589, -2.14738155, -0.25216984,  2.31366107,  6.52406372,\n",
    "        -4.88386469, -9.13828094, -0.71024376, -2.10295526, -1.27865374,\n",
    "         1.48217398,  1.43032329, -4.04631611,  5.72388976, -1.78674913]),\n",
    "    'C': np.array([\n",
    "        -8.36918251e+00,  8.30319345e+00, -6.61966946e+00,  1.38734140e+01,\n",
    "         8.59531324e+00,  9.79914818e+00,  1.26911187e+00, -4.63752901e+00,\n",
    "        -9.83921566e-01,  4.36342436e+00,  2.34365256e+00,  3.43445840e-01,\n",
    "         1.18290876e+00,  1.32558804e-02, -6.96621503e-01]),\n",
    "    'E': np.array([\n",
    "        12.03159874, -13.30121908,   8.27995382,  -2.1963886 ,\n",
    "         8.48308971,  -2.02004094,  -4.58165905,  -2.68526505,\n",
    "        -2.60424798,  -0.13228263,  -0.2394944 ,  -2.40393485,\n",
    "          4.1802745 ,  -4.40432039,   0.23979065]),\n",
    "    'Q': np.array([\n",
    "         7.9210965 , -8.69666506, -0.59701694,  0.02046296,  0.93026397,\n",
    "         2.56565474,  1.14554766,  0.50500247,  0.71467344,  0.04438606,\n",
    "        -6.00954372,  3.89329376,  2.44305793, -2.76002278, -1.6987172 ]),\n",
    "    'G': np.array([\n",
    "        14.83920873, 19.24138906,  5.93438829,  5.66738401, -5.81883134,\n",
    "        -8.78866739,  5.73914235, -4.72849218, -4.20333219, -1.56492845,\n",
    "        -1.37450169, -0.61253836,  0.6617134 , -0.69921372, -1.40639534]),\n",
    "    'H': np.array([\n",
    "         0.68805439, -6.17956596, -6.80550604,  3.94746531,  1.33297594,\n",
    "        -0.51211304,  4.73783089,  8.74722377, -3.12401236, -1.57598186,\n",
    "         1.98977266, -7.78869789,  0.99852322,  1.37575734, -0.43120018]),\n",
    "    'I': np.array([\n",
    "        -20.34084677,   4.14384693,   3.86394279,  -3.40504491,\n",
    "         -2.78992754,   1.46082574,  -4.07088528,   0.58387152,\n",
    "         -3.41337954,   1.84625632,  -3.12082912,  -1.14873131,\n",
    "         -4.65093114,  -1.64689363,   1.46078507]),\n",
    "    'L': np.array([\n",
    "        -17.63623772,  -0.35239597,  11.83924317,  -5.37540851,\n",
    "         -1.15954659,  -1.97240237,   0.8551395 ,   0.5305574 ,\n",
    "          2.1201762 ,   4.05167588,   7.06745699,   2.57547229,\n",
    "          1.4547904 ,   1.28288187,  -0.26554788]),\n",
    "    'K': np.array([\n",
    "        11.70658851, -13.64488577,   2.13621858,  -2.01605285,\n",
    "        -6.38068619,   0.81226081,   4.37857192,  -1.6496082 ,\n",
    "         1.42540472,   8.6009414 ,  -2.9141596 ,  -1.63197862,\n",
    "        -1.76542326,   2.93593542,  -2.78651305]),\n",
    "    'M': np.array([\n",
    "        -15.59654594,  -5.74596901,   1.05654898,   1.8258127 ,\n",
    "          6.65236588,  -1.21996347,   6.87749571,   2.80480871,\n",
    "         -1.90746691,  -3.59099156,  -4.0585946 ,   5.83997368,\n",
    "         -2.03857504,   2.47274257,   1.55383649]),\n",
    "    'F': np.array([\n",
    "        -18.59572859,   0.92235509,  -3.31743268,  -2.52174562,\n",
    "         -0.45125923,  -4.09153376,  -0.39932096,   2.4112244 ,\n",
    "         -1.0836413 ,  -1.47072537,   3.19432868,   2.04374527,\n",
    "          0.29441083,  -3.18840009,  -6.45603094]),\n",
    "    'P': np.array([\n",
    "        16.21704723,  15.09127036,  -8.72320617, -18.77750006,\n",
    "         6.46478124,   4.56244976,   3.05557799,   0.15690408,\n",
    "        -0.66929999,   0.7338994 ,   0.3953321 ,  -0.16512658,\n",
    "        -0.43811542,   0.05203475,   0.50555428]),\n",
    "    'S': np.array([\n",
    "        11.85274099,  6.88460214,  2.96725951,  3.6290185 , -2.88619312,\n",
    "         2.32382186, -1.33830204,  3.31676758,  6.26373175, -0.89789266,\n",
    "         0.739414  ,  1.15021805,  1.36104635,  1.13305848,  1.16395609]),\n",
    "    'T': np.array([\n",
    "         4.53029865,  5.12654936,  1.43605965,  1.7684372 , -3.39334852,\n",
    "         5.24360932, -3.36448143,  2.20062621,  6.52685307, -4.74609137,\n",
    "        -2.03988324, -0.65055289,  0.19272884, -0.25439792, -3.25573314]),\n",
    "    'W': np.array([\n",
    "        -16.29768602,  -3.90769937, -13.79255488,  -0.84325799,\n",
    "          2.33264713,  -8.04187276,   0.50186494,  -6.42108915,\n",
    "          7.2327353 ,  -1.12521691,  -1.01561524,  -3.54186466,\n",
    "         -1.9445611 ,  -0.81438463,   1.37250825]),\n",
    "    'Y': np.array([\n",
    "        -7.49926   ,   1.31004507, -12.07792238,  -1.16106141,\n",
    "        -6.8946393 ,  -3.23103968,  -4.86248111,   0.68815174,\n",
    "        -2.45473764,   1.48233974,  -1.42869414,   2.18166427,\n",
    "         7.4468281 ,   3.0783195 ,   2.84874949]),\n",
    "    'V': np.array([\n",
    "        -16.01441319,   5.49304582,   7.34505768,  -0.71258533,\n",
    "         -4.03171245,   5.60745006,  -4.73473404,  -0.44655998,\n",
    "         -2.68117513,  -0.63176764,  -2.98159019,  -3.67133046,\n",
    "         -1.09976811,   0.24198258,   0.76951331])}\n",
    "\n",
    "def enc_list_bl_max_len(aa_seqs, aa_codes, max_seq_len):\n",
    "    '''\n",
    "    aa_codes of a list of amino acid sequences with padding \n",
    "    to a max length\n",
    "\n",
    "    parameters:\n",
    "        - aa_seqs : list with AA sequences\n",
    "        - aa_codes : dictionnary: key= AA, value= aa_codes\n",
    "        - max_seq_len: common length for padding\n",
    "    returns:\n",
    "        - enc_aa_seq : list of np.ndarrays containing padded, encoded amino acid sequences\n",
    "    '''\n",
    "\n",
    "    # encode sequences:\n",
    "    sequences=[]\n",
    "    for seq in aa_seqs:\n",
    "        e_seq=np.zeros((len(seq),len(aa_codes[\"A\"])), dtype=np.int32)\n",
    "        count=0\n",
    "        for aa in seq:\n",
    "            if aa in aa_codes:\n",
    "                e_seq[count]=aa_codes[aa]\n",
    "                count+=1\n",
    "            else:\n",
    "                sys.stderr.write(\"Unknown amino acid in peptides: \"+ aa +\", encoding aborted!\\n\")\n",
    "                sys.exit(2)\n",
    "                \n",
    "        sequences.append(e_seq)\n",
    "\n",
    "    # pad sequences:\n",
    "    #max_seq_len = max([len(x) for x in aa_seqs])\n",
    "    \n",
    "    n_seqs = len(aa_seqs)\n",
    "    n_features = sequences[0].shape[1]\n",
    "\n",
    "    enc_aa_seq = np.zeros((n_seqs, max_seq_len, n_features), dtype=np.float32)\n",
    "    for i in range(0,n_seqs):\n",
    "        enc_aa_seq[i, :sequences[i].shape[0], :n_features] = sequences[i]\n",
    "\n",
    "    return enc_aa_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb8285",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### HLA pseudo-sequence ##########################\n",
    "\n",
    "HLA_ABC=[hla_db_dir+'/A_prot.fasta',hla_db_dir+'/B_prot.fasta',hla_db_dir+'/C_prot.fasta',hla_db_dir+'/E_prot.fasta']\n",
    "# HLA_ABC=[hla_db_dir+'/AA.txt']\n",
    "HLA_seq_lib = {}\n",
    "\n",
    "# 遍历 HLA_ABC 中的文件路径\n",
    "for one_class in HLA_ABC:\n",
    "    with open(one_class, 'r') as prot:\n",
    "        name = ''\n",
    "        sequence = ''\n",
    "        for line in prot:\n",
    "            if line.startswith('>HLA'):\n",
    "                if name and sequence:\n",
    "                    HLA_seq_lib[name] = sequence\n",
    "                name = line.split(' ')[1]\n",
    "                sequence = ''\n",
    "            else:\n",
    "                sequence += line.strip()\n",
    "        \n",
    "        # 处理最后一个 HLA\n",
    "        if name and sequence:\n",
    "            HLA_seq_lib[name] = sequence\n",
    "\n",
    "\n",
    "def HLA_seq_list(HLA):\n",
    "    hla_list = []\n",
    "\n",
    "    for HLA_name in HLA:\n",
    "        if HLA_name not in HLA_seq_lib.keys():\n",
    "            if len([hla_allele for hla_allele in HLA_seq_lib.keys() if hla_allele.startswith(str(HLA_name))]) == 0:\n",
    "                print('cannot find' + HLA_name)\n",
    "            HLA_name = [hla_allele for hla_allele in HLA_seq_lib.keys() if hla_allele.startswith(str(HLA_name))][0]\n",
    "\n",
    "        if HLA_name not in HLA_seq_lib.keys():\n",
    "            print('Not proper HLA allele:' + HLA_name)\n",
    "\n",
    "        HLA_sequence = HLA_seq_lib[HLA_name]\n",
    "        hla_list.append(HLA_sequence)\n",
    "\n",
    "    return hla_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e89428-a811-427a-851e-3a180a701ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################  LSTM module ##########################\n",
    "\n",
    "class DotProductScore(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(DotProductScore, self).__init__()\n",
    "        self.q = nn.Parameter(torch.empty(size=(hidden_size, 1), dtype=torch.float32))\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Initialize weight range\n",
    "        initrange = 0.5\n",
    "        self.q.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            - X: Input matrix, inputs=[batch_size, seq_length, hidden_size]\n",
    "        Output:\n",
    "            - scores: Output matrix, shape=[batch_size, seq_length]\n",
    "        \"\"\"\n",
    "        # Calculate attention scores using dot product attention\n",
    "        scores = torch.matmul(inputs, self.q)\n",
    "        # Compress the last dimension of the tensor, from (batch_size, seq_length, 1) to (batch_size, seq_length)\n",
    "        scores = scores.squeeze(-1)\n",
    "        return scores\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        # Define the attention module, which includes the DotProductScore module\n",
    "        self.scores = DotProductScore(hidden_size)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        # Calculate attention scores ((batch_size, seq_length))\n",
    "        scores = self.scores(X)\n",
    "        # Generate masks to mask positions beyond valid lengths\n",
    "        arrange = torch.arange(X.size(1), dtype=torch.float32, device=X.device).unsqueeze(0)\n",
    "        mask = (arrange < valid_lens.unsqueeze(-1)).float()\n",
    "        # Set scores of invalid positions to negative infinity to make them approach zero in softmax operation\n",
    "        scores = scores * mask - (1 - mask) * 1e9  # Mask invalid positions\n",
    "        # Use softmax to get attention weights\n",
    "        attention_weights = nn.functional.softmax(scores, dim=-1)\n",
    "        # Apply attention weights to input tensor X to get the weighted average output\n",
    "        out = torch.matmul(attention_weights.unsqueeze(1), X).squeeze(1)\n",
    "        return out\n",
    "\n",
    "class ModelLSTMAttention(nn.Module):\n",
    "    def __init__(self, input_size=15, hidden_size=30, output_size=64, num_layers=2, dropout=0.4):\n",
    "        super(ModelLSTMAttention, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        # Define the attention module, where the input hidden_size needs to be multiplied by 2 because it is bidirectional\n",
    "        self.attention = Attention(hidden_size * 2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Define the final fully connected layer, the input dimension is also hidden_size * 2\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, seq, valid_lens):\n",
    "        \n",
    "        output, _ = self.lstm(seq)\n",
    "        valid_lens = valid_lens.view(-1,).to(device)\n",
    "        # Apply the attention module to get the weighted average output\n",
    "        out = self.attention(output, valid_lens)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30927c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################  Transformer module ##########################\n",
    "\n",
    "sys.path.append(\"./python/\")\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_steps, max_dims, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        if max_dims % 2 == 1: max_dims += 1\n",
    "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
    "        pos_emb = np.empty((1, max_steps, max_dims))\n",
    "        pos_emb[0, :,  ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
    "        self.positional_embeddding = torch.tensor(pos_emb, dtype=dtype)\n",
    "        #positional_embeddding = torch.tensor(pos_emb, dtype=dtype)\n",
    "        #self.register_buffer('positional_embeddding', positional_embeddding)\n",
    "    def forward(self, inputs):\n",
    "        shape = inputs.shape\n",
    "        return inputs + self.positional_embeddding[:, :shape[-2], :shape[-1]]\n",
    "      \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "#         self.sparsemax = Sparsemax(dim = 1)\n",
    "        \n",
    "        assert embed_dim % self.num_heads == 0\n",
    "        self.depth = embed_dim // self.num_heads\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.depth]))\n",
    "        #scale = torch.sqrt(torch.FloatTensor([self.depth]))\n",
    "        #self.register_buffer('scale', scale)\n",
    "        self.wq = nn.Linear(embed_dim, embed_dim)\n",
    "        self.wk = nn.Linear(embed_dim, embed_dim)\n",
    "        self.wv = nn.Linear(embed_dim, embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.shape[0] # \n",
    "        \n",
    "        # (batch_size, seq_len, embed_dim)\n",
    "        Q = self.wq(q)  \n",
    "        K = self.wk(k)\n",
    "        V = self.wv(v)\n",
    "        \n",
    "        # (batch_size, num_heads, seq_len_q, depth) \n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.depth).permute(0,2,1,3)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.depth).permute(0,2,1,3)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.depth).permute(0,2,1,3)\n",
    "        \n",
    "        attention = torch.matmul(Q, K.permute(0,1,3,2)) / self.scale.to(device)\n",
    "        \n",
    "        \n",
    " \n",
    "        \n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask==0, -1e10)\n",
    "        attention = torch.softmax(attention, dim=-1).to(device)\n",
    "        \n",
    "        x = torch.matmul(attention, V)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous() # (batch_size, seq_len_q, num_heads, depth)\n",
    "        x = x.view(batch_size, -1, self.embed_dim) # (batch_size, seq_len, embed_dim)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "      \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads).to(device)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.mha(x,x,x)\n",
    "        out1 = self.dropout1(out1)\n",
    "        out1 = self.layernorm1(x+out1)\n",
    "        out2 = self.ffn(out1)\n",
    "        out2 = self.dropout2(out2)\n",
    "        out2 = self.layernorm2(out1+out2)\n",
    "        return out2\n",
    "class TokAndPosEmbedding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=9, voc_size=21):\n",
    "        super().__init__()\n",
    "#         self.pos = torch.arange(max_len)\n",
    "        self.pos = torch.arange(max_len).to(device)\n",
    "        #pos = torch.arange(max_len)\n",
    "        #self.register_buffer('pos', pos)\n",
    "        self.tok_embedding = nn.Embedding(voc_size, embed_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_len, embed_dim).to(device)\n",
    "    def forward(self, x):\n",
    "        return self.tok_embedding(x) + self.pos_embedding(self.pos)\n",
    "\n",
    "class AttenCaldX(nn.Module):\n",
    "    def __init__(self, embed_dim=16, num_heads=4, ff_dim=64, rate=0.1, max_len=12):\n",
    "        super().__init__()\n",
    "        self.emb = TokAndPosEmbedding(embed_dim, max_len, 21)\n",
    "        self.encoder1 = Encoder(embed_dim, num_heads, ff_dim, rate)\n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "        self.encoder2 = Encoder(embed_dim, num_heads, ff_dim, rate)\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "        self.dropout3 = nn.Dropout(rate)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(embed_dim*2, 64)\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.emb(x)\n",
    "        x = self.encoder1(x)\n",
    "        x = self.dropout1(x)\n",
    "        y = self.encoder2(x)\n",
    "        y = self.dropout2(y)\n",
    "        #out = x + y\n",
    "        out = torch.concat([x,y], dim=2)\n",
    "        out = out.permute(0,2,1)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(batch_size, -1)\n",
    "        out = self.dropout3(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd522d-38c8-4763-bb56-4481e3288519",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################  model ##########################\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # Initializing model components\n",
    "        self.pep_extractor = AttenCaldX()\n",
    "        self.hla_extractor = ModelLSTMAttention()\n",
    "\n",
    "        # Defining convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(2,1)),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 1)),\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(1,4)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 1)),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(1,4)),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "        )\n",
    "\n",
    "        # Defining fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(1792, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.4),\n",
    "            nn.Linear(100, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, pep, hla_seq, hla_len):\n",
    "        # Extracting features\n",
    "        output1 = self.pep_extractor(pep)\n",
    "        output2 = self.hla_extractor(hla_seq, hla_len)\n",
    "        # Concatenating features\n",
    "        x = torch.cat((output1.unsqueeze(1), output2.unsqueeze(1)), dim=1)\n",
    "        x = x.unsqueeze(1)  # Adding a dimension as channel\n",
    "        # Convolutional layers\n",
    "        x = self.conv_layers(x)\n",
    "        # Flattening features\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Fully connected layers\n",
    "        x = self.fc_layers(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b126a03-be96-4cba-9741-a12830cb23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the function to compute metrics\n",
    "def compute_metrics(y_labels, y_preds, cutoff=0.5, digit=4):\n",
    "    if (cutoff <= 0) or (cutoff >= 1):\n",
    "        fpr, tpr, threshold = roc_curve(y_labels, y_preds)\n",
    "        cutoff = sorted(list(zip(np.abs(tpr-fpr), threshold)), key=lambda i: i[0], reverse=True)[0][1]\n",
    "    \n",
    "    y_pred_labels = np.array([1. if p >= cutoff else 0. for p in y_preds])\n",
    "    Accuracy = accuracy_score(y_labels, y_pred_labels)\n",
    "    Recall = recall_score(y_labels, y_pred_labels)\n",
    "    \n",
    "    # Compute specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y_labels, y_pred_labels).ravel()\n",
    "    Specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    \n",
    "    F1 = f1_score(y_labels, y_pred_labels)\n",
    "    AUC = roc_auc_score(y_labels, y_preds)\n",
    "    spearman_corr, _ = spearmanr(y_labels, y_preds)\n",
    "    \n",
    "    # Calculate FPR and TPR\n",
    "    fpr, tpr, _ = roc_curve(y_labels, y_preds)\n",
    "    \n",
    "    return {'Accuracy': np.round(Accuracy, digit),\n",
    "            'Sensitivity': np.round(Recall, digit),\n",
    "            'Specificity': np.round(Specificity, digit),\n",
    "            'Threshold': np.round(cutoff, digit),\n",
    "            'F1': np.round(F1, digit),\n",
    "            'AUC': np.round(AUC, digit),\n",
    "            'SRCC':  np.round(spearman_corr,digit),\n",
    "            \n",
    "            'FPR': fpr,\n",
    "            'TPR': tpr}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9ecffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Load the HLA dataset\n",
    "def load_hla_dataframe(fname):\n",
    "    df = pd.read_table(fname) \n",
    "    # remove peptides with ambiguous symbols: X, B\n",
    "    df = df[df.Peptide.str.contains('X|B') == False]\n",
    "    return df\n",
    "\n",
    "def get_hla_subtype(df, min_len=9, max_len=12):\n",
    "    df = df[\n",
    "             (df['Peptide'].str.len() >= min_len) &\n",
    "             (df['Peptide'].str.len() <= max_len)]\n",
    "    return df\n",
    "\n",
    "\n",
    "def vectorize_peps(peps, max_len=12):\n",
    "    num_pep = len(peps)\n",
    "    X = np.zeros((num_pep, max_len), dtype=np.int32)\n",
    "    for i in range(num_pep):\n",
    "        aa_code_seq = [aa_dict[aa] for aa in peps[i]]\n",
    "        pep_len = len(aa_code_seq)\n",
    "        assert pep_len <= max_len\n",
    "        X[i, max_len-pep_len:]=np.array(aa_code_seq)\n",
    "        pass\n",
    "    return np.array(X, dtype=np.int32)\n",
    "    \n",
    "class HLADataset(Dataset):\n",
    "    def __init__(self, peptide, hla,lengths, label):\n",
    "        self.peptide = peptide\n",
    "        self.hla = hla\n",
    "        self.label = label\n",
    "        self.lengths=lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.peptide[i], self.hla[i], self.lengths[i],self.label[i]\n",
    "\n",
    "\n",
    "\n",
    "#Learning rate scheduling\n",
    "def adjust_learning_rate(epoch):\n",
    "    if epoch < 100:\n",
    "      return 0.0001\n",
    "    elif epoch < 200:\n",
    "      return 0.00005\n",
    "    elif epoch < 250:\n",
    "      return 0.00001\n",
    "    else:\n",
    "      return 0.000001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec0aa0-1905-43f5-91a7-e9b86d489697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(df, BATCH_SIZE=64):\n",
    "    # Vectorize peptide sequences\n",
    "    Peptide = vectorize_peps(list(df.Peptide))\n",
    "    # Create labels\n",
    "    label = np.array(\n",
    "        [1 if x==1 else 0 for x in df.BindingCategory], dtype=np.float32).reshape(-1,1)\n",
    "    print(label.shape)\n",
    "    print(Peptide.shape)\n",
    "\n",
    "    # Extract HLA sequences\n",
    "    hla = df.HLA\n",
    "    HLA_sequence = []\n",
    "    HLA_sequence = HLA_seq_list(hla)\n",
    "    lengths_HLA_sequence = [len(sequence) for sequence in HLA_sequence]\n",
    "    # Encode HLA sequences\n",
    "    HLA_seq = enc_list_bl_max_len(HLA_sequence, pca15_aaidx, 366)\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    Peptide_train, Peptide_test, HLA_train, HLA_test, lengths_train, lengths_test, label_train, label_test = train_test_split(\n",
    "         Peptide, HLA_seq, lengths_HLA_sequence, label, test_size=0.2, random_state=2024)\n",
    "    print(\"X_train shape:\", Peptide_train.shape)\n",
    "    print(\"X_test shape:\", Peptide_test.shape)\n",
    "    print(\"Z_train shape:\", HLA_train.shape)\n",
    "    print(\"Z_test shape:\", HLA_test.shape)\n",
    "    print(\"y_train shape:\", label_train.shape)\n",
    "    print(\"y_test shape:\", label_test.shape)\n",
    "\n",
    "    # Convert to tensors\n",
    "    Peptide_train = list(Peptide_train)\n",
    "    Peptide_test = list(Peptide_test)\n",
    "    HLA_train = list(HLA_train)\n",
    "    HLA_test = list(HLA_test)\n",
    "\n",
    "    Peptide_train = torch.tensor(Peptide_train)\n",
    "    Peptide_test = torch.tensor(Peptide_test)\n",
    "    lengths_train = torch.tensor(lengths_train)\n",
    "    lengths_test = torch.tensor(lengths_test)\n",
    "    hla_train = torch.tensor(HLA_train)\n",
    "    hla_test = torch.tensor(HLA_test)\n",
    "    label_train = torch.tensor(label_train)\n",
    "    label_test = torch.tensor(label_test)\n",
    "\n",
    "    # Create datasets\n",
    "    train_data = HLADataset(Peptide_train, hla_train, lengths_train, label_train)\n",
    "    test_data = HLADataset(Peptide_test, hla_test, lengths_test, label_test)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_data, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "    return train_data, test_data, train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff63f16-6753-4e59-805c-354a6d995e7e",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ab5d8-dbe9-458b-9e52-c8667c3d83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_network(model,train_data, test_data,train_loader, test_loader, loss_fn, NUM_EPOCHS=300,cur_lr = 0.0001,BATCH_SIZE=64):\n",
    "\n",
    "    #Count the number of iterations of the training and test datasets in each round\n",
    "    trainSteps = len(train_data) // BATCH_SIZE\n",
    "    testSteps = len(test_data) // BATCH_SIZE\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cur_lr) # Optimizer\n",
    "    \n",
    "    # Initialize dictionary to record history data\n",
    "    H = {\"train_loss\": [], \"test_loss\": [], \"train_acc\": [], \"test_acc\": [],\n",
    "         \"train_auc\": [], \"test_auc\": [], \"train_sensitivity\": [], \"test_sensitivity\": [],\n",
    "         \"train_specificity\": [], \"test_specificity\": [], \"train_f1\": [], \"test_f1\": [],\n",
    "         \"train_SRCC\": [], \"test_SRCC\": []}\n",
    "\n",
    "    best_auc = 0.0\n",
    "    best_epoch = 0\n",
    "\n",
    "    # Initialize arrays to store ROC data\n",
    "    roc_fpr_train = np.array([])\n",
    "    roc_tpr_train = np.array([])\n",
    "    roc_fpr_test = np.array([])\n",
    "    roc_tpr_test = np.array([])\n",
    "\n",
    "    startTime = time.time()\n",
    "    for e in tqdm(range(NUM_EPOCHS)):\n",
    "        lr = adjust_learning_rate(e)\n",
    "        if cur_lr != lr:\n",
    "            cur_lr = lr\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=cur_lr)\n",
    "\n",
    "        # Set model to training mode\n",
    "        \n",
    "        model.train()\n",
    "        totalTrainLoss = 0\n",
    "        totalTestLoss = 0\n",
    "\n",
    "        # Initialize total predictions and true labels\n",
    "        all_train_predictions = []\n",
    "        all_train_targets = []\n",
    "\n",
    "\n",
    "        # Iterate over training dataset\n",
    "        for (i, (x, z, l, y)) in enumerate(train_loader):\n",
    "            (x, z, l, y) = (x.to(device), z.to(device), l.to(device), y.to(device))\n",
    "\n",
    "            # Forward pass and compute loss\n",
    "            pred = model(x, z, l)\n",
    "            loss = loss_fn(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Zero gradients, backward pass, and update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Add loss to total training loss\n",
    "            totalTrainLoss += loss\n",
    "\n",
    "            # Save predictions and target labels for training ACC and AUC calculation\n",
    "            all_train_predictions.extend(pred.cpu().detach().numpy())\n",
    "            all_train_targets.extend(y.cpu().detach().numpy())\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Initialize total predictions and true labels for testing\n",
    "        all_test_predictions = []\n",
    "        all_test_targets = []\n",
    "\n",
    "        # Iterate over testing dataset\n",
    "        for (x1, z1, l1, y1) in test_loader:\n",
    "            (x1, z1, l1, y1) = (x1.to(device), z1.to(device), l1.to(device), y1.to(device))\n",
    "            pred1 = model(x1, z1, l1)\n",
    "            totalTestLoss += loss_fn(pred1, y1)\n",
    "\n",
    "            # Save predictions and target labels for testing ACC and AUC calculation\n",
    "            all_test_predictions.extend(pred1.cpu().detach().numpy())\n",
    "            all_test_targets.extend(y1.cpu().detach().numpy())\n",
    "\n",
    "        # Calculate average training loss and testing loss\n",
    "        avgTrainLoss = totalTrainLoss / trainSteps\n",
    "        avgTestLoss = totalTestLoss / testSteps\n",
    "\n",
    "        # Calculate training and testing metrics\n",
    "        train_metrics = compute_metrics(all_train_targets, all_train_predictions)\n",
    "        test_metrics = compute_metrics(all_test_targets, all_test_predictions)\n",
    "\n",
    "        # Update training history data\n",
    "        H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "        H[\"test_loss\"].append(avgTestLoss.cpu().detach().numpy())\n",
    "        H[\"train_acc\"].append(train_metrics['Accuracy'])\n",
    "        H[\"test_acc\"].append(test_metrics['Accuracy'])\n",
    "        H[\"train_auc\"].append(train_metrics['AUC'])\n",
    "        H[\"test_auc\"].append(test_metrics['AUC'])\n",
    "        H[\"train_sensitivity\"].append(train_metrics['Sensitivity'])\n",
    "        H[\"test_sensitivity\"].append(test_metrics['Sensitivity'])\n",
    "        H[\"train_specificity\"].append(train_metrics['Specificity'])\n",
    "        H[\"test_specificity\"].append(test_metrics['Specificity'])\n",
    "        H[\"train_f1\"].append(train_metrics['F1'])\n",
    "        H[\"test_f1\"].append(test_metrics['F1'])\n",
    "        H[\"train_SRCC\"].append(train_metrics['SRCC'])\n",
    "        H[\"test_SRCC\"].append(test_metrics['SRCC'])\n",
    "\n",
    "        \n",
    "        # Print training information for the current epoch\n",
    "        print(\"[INFO] Epoch: {}/{}, LR: {:.6f}\".format(e + 1, NUM_EPOCHS, cur_lr))\n",
    "        print(\"Train Loss: {:.4f}, Test Loss: {:.4f}\".format(avgTrainLoss, avgTestLoss))\n",
    "        print(\"Train ACC: {:.3f}, Test ACC: {:.3f}\".format(train_metrics['Accuracy'], test_metrics['Accuracy']))\n",
    "        print(\"Train AUC: {:.3f}, Test AUC: {:.3f}\".format(train_metrics['AUC'], test_metrics['AUC']))\n",
    "        print(\"Train Sensitivity: {:.3f}, Test Sensitivity: {:.3f}\".format(train_metrics['Sensitivity'], test_metrics['Sensitivity']))\n",
    "        print(\"Train Specificity: {:.3f}, Test Specificity: {:.3f}\".format(train_metrics['Specificity'], test_metrics['Specificity']))\n",
    "        print(\"Train F1: {:.3f}, Test F1: {:.3f}\".format(train_metrics['F1'], test_metrics['F1']))\n",
    "        print(\"Train SRCC: {:.3f}, Test SRCC: {:.3f}\".format(train_metrics['SRCC'], test_metrics['F1']))\n",
    "\n",
    "\n",
    "        # Update best AUC and epoch\n",
    "        if test_metrics['AUC'] > best_auc:\n",
    "            best_auc = test_metrics['AUC']\n",
    "            best_epoch = e\n",
    "            best_auc_trein = train_metrics['AUC']\n",
    "\n",
    "            roc_fpr_train = train_metrics['FPR']\n",
    "            roc_tpr_train = train_metrics['TPR']\n",
    "            roc_fpr_test = test_metrics['FPR']\n",
    "            roc_tpr_test = test_metrics['TPR']\n",
    "\n",
    "    # Print total time taken for training\n",
    "    endTime = time.time()\n",
    "    print(\"[INFO] Total time taken to train the model: {:.2f}s\".format(endTime - startTime))\n",
    "\n",
    "    #Save the model\n",
    "    torch.save(model.state_dict(), '../model/TlcMHCpan_model.pth')\n",
    "    return  best_auc,best_auc_trein, best_epoch, roc_fpr_train, roc_tpr_train, roc_fpr_test, roc_tpr_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103be9e6-2bee-4cbb-b798-8805e05571be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(train_data_path, model, loss_fn, device=\"cuda\"):\n",
    "    # Load and process the data\n",
    "    df_data = load_hla_dataframe(train_data_path)\n",
    "    df_data = get_hla_subtype(df_data)\n",
    "    # df_data1 = df_data[:5000]\n",
    "    train_data, test_data, train_loader, test_loader = data_processing(df_data)\n",
    "\n",
    "    # Move the model to the specified device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Train the network\n",
    "    best_auc, best_auc_train, best_epoch, roc_fpr_train, roc_tpr_train, roc_fpr_test, roc_tpr_test = train_network(model, train_data, test_data, train_loader, test_loader, loss_fn)\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(roc_fpr_train, roc_tpr_train, label='Train ROC curve (area = %0.3f)' % best_auc_train, color='darkorange')\n",
    "    plt.plot(roc_fpr_test, roc_tpr_test, label='Test ROC curve (area = %0.3f)' % best_auc, color='blue')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='black')\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('PROTEINS Dataset')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save ROC curve plot as an image\n",
    "    plt.savefig(\"PROTEINS_ROC.png\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Best AUC: {:.3f} at Epoch: {} and train AUC: {:.3f}\".format(best_auc, best_epoch + 1, best_auc_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f59243-474b-4285-88af-df3ade97e6ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device = \"cuda\"\n",
    "# Initialize the model and move it to the specified device\n",
    "model = Model().to(device)\n",
    "# Loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_data_path = '../data/proteins.txt'  # Path to the training data\n",
    "\n",
    "train_and_evaluate_model(train_data_path, model, loss_fn)  # Call the function to train and evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb6cd70-36b9-447a-b307-ab29e9689639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
